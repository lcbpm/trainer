# 网络技术简明笔记

## 目录
- [1. 网络5层模型](#1-网络5层模型)
- [2. Socket 与网络层关系](#2-socket-与网络层关系)
- [3. 零拷贝（Zero-Copy）](#3-零拷贝zero-copy)
  - [各技术对比](#各技术对比)
- [4. 直接内存与零拷贝的关系](#4-直接内存与零拷贝的关系)
  - [直接内存如何支持零拷贝](#直接内存如何支持零拷贝)
  - [直接内存与各种零拷贝技术的结合](#直接内存与各种零拷贝技术的结合)
  - [实际应用场景](#实际应用场景)
- [5. Epoll 与高并发](#5-epoll-与高并发)
  - [与 Select/Poll 对比](#与-selectpoll-对比)
  - [为什么 Epoll 高性能](#为什么-epoll-高性能)
- [6. 与其他模型关系](#6-与其他模型关系)
- [7. 数据传输过程](#7-数据传输过程)

## 1. 网络5层模型

| 层级 | 名称    | 功能        | 示例       |
| -- | ----- | --------- | -------- |
| 1  | 物理层   | 比特流传输     | 电缆、光纤    |
| 2  | 数据链路层 | 帧传输、MAC寻址 | 以太网、WiFi |
| 3  | 网络层   | 路由、逻辑地址   | IP、路由器   |
| 4  | 传输层   | 端到端通信     | TCP、UDP  |
| 5  | 应用层   | 提供服务接口    | HTTP、DNS |

---

## 2. Socket 与网络层关系

* **位置**：应用层与传输层之间的接口
* **作用**：

  * 封装 TCP/UDP 复杂性
  * 提供统一 API（connect、send、recv）
* **关系**：

  * 应用层调用 Socket
  * Socket 调用 TCP/UDP
  * 内核处理 IP/链路层/物理层

---

## 3. 零拷贝（Zero-Copy）

**目标**：减少 CPU 拷贝，减少用户态/内核态切换。

### 各技术对比

| 技术                | CPU拷贝 | 系统调用 | 特点        |
| ----------------- | ----- | ---- | --------- |
| read+write        | 2次    | 2次   | 最传统       |
| mmap+write        | 1次    | 2次   | 消除内核→用户拷贝 |
| sendfile          | 1次    | 1次   | 完全在内核完成   |
| sendfile + DMA SG | 0次    | 1次   | 真零拷贝      |

---

## 4. 直接内存与零拷贝的关系

**直接内存**（Direct Memory）是JVM堆外的内存区域，专门为了解决传统I/O操作中的性能瓶颈而设计。

### 直接内存如何支持零拷贝

1. **减少内存拷贝层级**：
   - 传统I/O：磁盘 → 内核缓冲区 → JVM堆内存 → 应用程序变量（3次拷贝）
   - 直接内存I/O：磁盘 → 内核缓冲区 → 直接内存 → 应用程序（2次拷贝）

2. **绕过JVM堆**：
   - 直接内存不在JVM堆中，避免了用户空间与内核空间的数据转换
   - 操作系统可直接访问直接内存，减少了JVM堆这一层拷贝

3. **为零拷贝技术提供基础**：
   - 直接内存为sendfile等零拷贝技术提供了高性能的数据缓冲区
   - 通过FileChannel.transferTo()等方法可实现真正的零拷贝传输

### 直接内存与各种零拷贝技术的结合

| 技术 | 传统内存 | 直接内存 | 零拷贝效果 |
|------|----------|----------|------------|
| read+write | 2次CPU拷贝 | 1次CPU拷贝 | 减少1次拷贝 |
| mmap+write | 1次CPU拷贝 | 1次CPU拷贝 | 无明显改善 |
| sendfile | 1次CPU拷贝 | 0次CPU拷贝 | 实现真零拷贝 |

### 实际应用场景

1. **高性能网络编程**：
   - Netty等框架大量使用直接内存提升网络I/O性能
   - 减少网络数据传输中的内存拷贝次数

2. **大文件处理**：
   - 处理大文件时减少GC压力
   - 提高I/O操作性能

3. **数据库和缓存系统**：
   - Redis等系统使用直接内存优化数据传输
   - 提高系统整体吞吐量

---

## 5. Epoll 与高并发

### 与 Select/Poll 对比

| 特性    | Select | Poll  | Epoll |
| ----- | ------ | ----- | ----- |
| fd数量  | 1024限制 | 无     | 无     |
| 时间复杂度 | O(n)   | O(n)  | O(1)  |
| 内存拷贝  | 每次都拷贝  | 每次都拷贝 | 注册一次  |
| 模型    | 轮询（固定集合） | 轮询（动态数组） | 事件驱动  |
| fd存储  | bitmap | 链表    | 红黑树   |

### 为什么 Epoll 高性能

* 内核回调，O(1) 获取就绪 fd
* 就绪队列，避免遍历全部 fd
* 内核数据结构只需注册一次
* 支持 ET（边缘触发）

---

## 6. 与其他模型关系

* **OSI 7层** → 物理/链路/网络/传输/会话/表示/应用
* **TCP/IP 4层** → 应用 / 传输 / 网际 / 网络接入
* **5层模型** → 教学简化版，兼顾二者

---

## 7. 数据传输过程

* **发送端**：逐层封装（应用→传输→网络→链路→物理）
* **接收端**：逐层解封装（物理→链路→网络→传输→应用）
* **单位**：

  * 应用层：Data
  * 传输层：Segment
  * 网络层：Packet
  * 链路层：Frame
  * 物理层：Bit


## 传统 `read + write`

1. **磁盘 → 内核缓冲区**（DMA，一次）
2. **内核缓冲区 → 用户缓冲区**（CPU 拷贝，一次）
3. **用户缓冲区 → 套接字缓冲区**（CPU 拷贝，一次）
4. **套接字缓冲区 → 网卡**（DMA，一次）

👉 总计：**2 次 DMA + 2 次 CPU 拷贝**

---

## `mmap + write`

1. **磁盘 → 内核缓冲区**（DMA，一次）
2. **用户态 mmap 共享内存**（**没有拷贝，只是建立映射**）
3. **用户缓冲区 → 套接字缓冲区**（CPU 拷贝，一次）
4. **套接字缓冲区 → 网卡**（DMA，一次）

👉 总计：**2 次 DMA + 1 次 CPU 拷贝**

---

## `sendfile`

> Linux 2.1 \~ 2.3 初期版本（还没优化前）

1. **磁盘 → 内核缓冲区**（DMA，一次）
2. **内核缓冲区 → 套接字缓冲区**（CPU 拷贝，一次）
3. **套接字缓冲区 → 网卡**（DMA，一次）

👉 总计：**2 次 DMA + 1 次 CPU 拷贝**
（和 mmap 差不多，但少了一次系统调用）

---

## `sendfile + DMA scatter/gather`（Linux 2.4+）

* 内核直接把 **磁盘缓冲区地址** + **socket 缓冲区描述符** 给 DMA
* DMA **直接把磁盘数据搬到网卡 buffer**
* CPU 只做描述符传递，不拷贝数据

👉 总计：**2 次 DMA + 0 次 CPU 拷贝**
（这才是“真正的零拷贝”）

---

✅ **回答你的问题**：

* **mmap 和早期 sendfile** → 都是 **CPU 一次拷贝，DMA 两次**
* **优化后的 sendfile (scatter/gather)** → **CPU 0 次拷贝，DMA 两次**

所以 **mmap 和 sendfile 都比传统 read/write 少一次 CPU 拷贝**，但 sendfile 还能进一步优化到真正的 **零拷贝**。
