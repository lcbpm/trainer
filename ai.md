# AI é«˜çº§é¢è¯•é¢˜å®Œæ•´æ•´ç†

## ğŸ“‹ ç›®å½•

- [1. æœºå™¨å­¦ä¹ åŸºç¡€](#1-æœºå™¨å­¦ä¹ åŸºç¡€)
  - [1.1 ç›‘ç£å­¦ä¹ ](#11-ç›‘ç£å­¦ä¹ )
  - [1.2 æ— ç›‘ç£å­¦ä¹ ](#12-æ— ç›‘ç£å­¦ä¹ )
  - [1.3 å¼ºåŒ–å­¦ä¹ ](#13-å¼ºåŒ–å­¦ä¹ )
  - [1.4 æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©](#14-æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©)
- [2. æ·±åº¦å­¦ä¹ ](#2-æ·±åº¦å­¦ä¹ )
  - [2.1 ç¥ç»ç½‘ç»œåŸºç¡€](#21-ç¥ç»ç½‘ç»œåŸºç¡€)
  - [2.2 å·ç§¯ç¥ç»ç½‘ç»œ](#22-å·ç§¯ç¥ç»ç½‘ç»œ)
  - [2.3 å¾ªç¯ç¥ç»ç½‘ç»œ](#23-å¾ªç¯ç¥ç»ç½‘ç»œ)
  - [2.4 Transformeræ¶æ„](#24-transformeræ¶æ„)
- [3. è‡ªç„¶è¯­è¨€å¤„ç†](#3-è‡ªç„¶è¯­è¨€å¤„ç†)
- [4. è®¡ç®—æœºè§†è§‰](#4-è®¡ç®—æœºè§†è§‰)
- [5. å¤§æ¨¡å‹ä¸LLM](#5-å¤§æ¨¡å‹ä¸llm)
- [6. AIå·¥ç¨‹å®è·µ](#6-aiå·¥ç¨‹å®è·µ)

---

## 1. æœºå™¨å­¦ä¹ åŸºç¡€

### 1.1 ç›‘ç£å­¦ä¹ 

**Q: è§£é‡Šåå·®(Bias)å’Œæ–¹å·®(Variance)çš„æ¦‚å¿µï¼Œä»¥åŠå®ƒä»¬ä¹‹é—´çš„æƒè¡¡ï¼Ÿ**

**ç†è®ºè§£æï¼š**
- **åå·®(Bias)**ï¼šæ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„ç³»ç»Ÿæ€§è¯¯å·®
- **æ–¹å·®(Variance)**ï¼šæ¨¡å‹åœ¨ä¸åŒè®­ç»ƒé›†ä¸Šé¢„æµ‹ç»“æœçš„å˜åŒ–ç¨‹åº¦
- **æƒè¡¡å…³ç³»**ï¼šæ€»è¯¯å·® = åå·®Â² + æ–¹å·® + å™ªå£°

**å®æˆ˜ä»£ç ç¤ºä¾‹ï¼š**
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

def bias_variance_analysis():
    """åå·®-æ–¹å·®åˆ†æå®æˆ˜æ¼”ç¤º"""
    
    # ç”ŸæˆçœŸå®å‡½æ•°æ•°æ®
    def true_function(x):
        return 1.5 * x**2 + 0.3 * x + 0.1
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    np.random.seed(42)
    n_samples = 100
    X = np.random.uniform(-1, 1, n_samples).reshape(-1, 1)
    noise = np.random.normal(0, 0.1, n_samples)
    y = true_function(X.ravel()) + noise
    
    # æµ‹è¯•æ•°æ®
    X_test = np.linspace(-1.2, 1.2, 100).reshape(-1, 1)
    y_test_true = true_function(X_test.ravel())
    
    # ä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
    degrees = [1, 3, 9, 15]
    n_experiments = 100
    
    results = {}
    
    for degree in degrees:
        predictions = []
        
        # å¤šæ¬¡å®éªŒæ¥ä¼°è®¡åå·®å’Œæ–¹å·®
        for i in range(n_experiments):
            # é‡æ–°é‡‡æ ·è®­ç»ƒæ•°æ®
            indices = np.random.choice(len(X), size=len(X), replace=True)
            X_bootstrap = X[indices]
            y_bootstrap = y[indices]
            
            # è®­ç»ƒæ¨¡å‹
            poly_model = Pipeline([
                ('poly', PolynomialFeatures(degree=degree)),
                ('linear', LinearRegression())
            ])
            
            poly_model.fit(X_bootstrap, y_bootstrap)
            pred = poly_model.predict(X_test)
            predictions.append(pred)
        
        predictions = np.array(predictions)
        
        # è®¡ç®—åå·®å’Œæ–¹å·®
        mean_prediction = np.mean(predictions, axis=0)
        bias_squared = np.mean((mean_prediction - y_test_true)**2)
        variance = np.mean(np.var(predictions, axis=0))
        
        results[degree] = {
            'bias_squared': bias_squared,
            'variance': variance,
            'total_error': bias_squared + variance,
        }
        
        print(f"å¤šé¡¹å¼åº¦æ•° {degree}:")
        print(f"  åå·®Â² = {bias_squared:.4f}")
        print(f"  æ–¹å·® = {variance:.4f}")
        print(f"  æ€»è¯¯å·® = {bias_squared + variance:.4f}")
        print()
    
    return results

# è¿è¡Œåˆ†æ
results = bias_variance_analysis()
```

**Q: ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆï¼Ÿå¦‚ä½•æ£€æµ‹å’Œè§£å†³ï¼Ÿ**

**æ£€æµ‹æ–¹æ³•ï¼š**
1. **éªŒè¯æ›²çº¿**ï¼šè§‚å¯Ÿè®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®éšå‚æ•°å˜åŒ–çš„è¶‹åŠ¿
2. **å­¦ä¹ æ›²çº¿**ï¼šè§‚å¯Ÿè¯¯å·®éšè®­ç»ƒæ ·æœ¬æ•°é‡å˜åŒ–çš„è¶‹åŠ¿
3. **äº¤å‰éªŒè¯**ï¼šä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›

**è§£å†³æ–¹æ¡ˆï¼š**
- **è¿‡æ‹Ÿåˆ**ï¼šæ­£åˆ™åŒ–ã€æ—©åœã€Dropoutã€æ•°æ®å¢å¼º
- **æ¬ æ‹Ÿåˆ**ï¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦ã€ç‰¹å¾å·¥ç¨‹ã€å‡å°‘æ­£åˆ™åŒ–

### 1.2 æ— ç›‘ç£å­¦ä¹ 

**Q: è§£é‡ŠK-meansèšç±»ç®—æ³•çš„åŸç†å’Œå±€é™æ€§ï¼Ÿ**

**K-meansç®—æ³•åŸç†ï¼š**
1. éšæœºåˆå§‹åŒ–Kä¸ªèšç±»ä¸­å¿ƒ
2. å°†æ¯ä¸ªç‚¹åˆ†é…åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
3. æ›´æ–°èšç±»ä¸­å¿ƒä¸ºè¯¥ç±»æ‰€æœ‰ç‚¹çš„å‡å€¼
4. é‡å¤2-3æ­¥ç›´åˆ°æ”¶æ•›

**å±€é™æ€§ï¼š**
- éœ€è¦é¢„å…ˆæŒ‡å®šKå€¼
- å¯¹åˆå§‹åŒ–æ•æ„Ÿ
- å‡è®¾èšç±»æ˜¯çƒå½¢çš„
- å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼æ•æ„Ÿ

**æ”¹è¿›æ–¹æ³•ï¼š**
```python
from sklearn.cluster import KMeans
import numpy as np

def improved_kmeans():
    """æ”¹è¿›çš„K-meanså®ç°"""
    
    # K-means++åˆå§‹åŒ–
    kmeans_plus = KMeans(n_clusters=3, init='k-means++', n_init=10)
    
    # å¤šæ¬¡è¿è¡Œé€‰æ‹©æœ€ä½³ç»“æœ
    best_inertia = float('inf')
    best_model = None
    
    for _ in range(20):
        kmeans = KMeans(n_clusters=3, random_state=None)
        kmeans.fit(X)
        if kmeans.inertia_ < best_inertia:
            best_inertia = kmeans.inertia_
            best_model = kmeans
    
    return best_model
```

### 1.3 å¼ºåŒ–å­¦ä¹ 

**Q: è§£é‡ŠQ-learningç®—æ³•çš„åŸç†ï¼Ÿ**

**Q-learningæ ¸å¿ƒå…¬å¼ï¼š**
```
Q(s,a) = Q(s,a) + Î±[r + Î³Â·max(Q(s',a')) - Q(s,a)]
```

**å…³é”®æ¦‚å¿µï¼š**
- **çŠ¶æ€(State)**ï¼šç¯å¢ƒçš„å½“å‰æƒ…å†µ
- **åŠ¨ä½œ(Action)**ï¼šæ™ºèƒ½ä½“å¯ä»¥é‡‡å–çš„è¡Œä¸º
- **å¥–åŠ±(Reward)**ï¼šæ‰§è¡ŒåŠ¨ä½œåè·å¾—çš„åé¦ˆ
- **Qå€¼**ï¼šåœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaçš„æœŸæœ›ç´¯ç§¯å¥–åŠ±

**ç®€åŒ–å®ç°ï¼š**
```python
class QLearning:
    def __init__(self, actions, lr=0.1, gamma=0.9, epsilon=0.1):
        self.actions = actions
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.q_table = {}
    
    def get_q_value(self, state, action):
        return self.q_table.get((state, action), 0.0)
    
    def update(self, state, action, reward, next_state):
        current_q = self.get_q_value(state, action)
        max_next_q = max([self.get_q_value(next_state, a) for a in self.actions])
        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)
        self.q_table[(state, action)] = new_q
    
    def choose_action(self, state):
        if random.random() < self.epsilon:
            return random.choice(self.actions)
        else:
            q_values = [self.get_q_value(state, a) for a in self.actions]
            return self.actions[np.argmax(q_values)]
```

### 1.4 æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©

**Q: è§£é‡Šäº¤å‰éªŒè¯çš„ç§ç±»å’Œé€‚ç”¨åœºæ™¯ï¼Ÿ**

**å¸¸è§äº¤å‰éªŒè¯æ–¹æ³•ï¼š**

1. **KæŠ˜äº¤å‰éªŒè¯**ï¼šé€‚ç”¨äºä¸€èˆ¬æƒ…å†µ
2. **åˆ†å±‚KæŠ˜**ï¼šé€‚ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹
3. **ç•™ä¸€æ³•(LOOCV)**ï¼šé€‚ç”¨äºå°æ•°æ®é›†
4. **æ—¶é—´åºåˆ—äº¤å‰éªŒè¯**ï¼šé€‚ç”¨äºæ—¶åºæ•°æ®

```python
from sklearn.model_selection import (
    KFold, StratifiedKFold, LeaveOneOut, TimeSeriesSplit
)

def cross_validation_demo():
    """äº¤å‰éªŒè¯æ–¹æ³•æ¼”ç¤º"""
    
    # KæŠ˜äº¤å‰éªŒè¯
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    # åˆ†å±‚KæŠ˜
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
    tscv = TimeSeriesSplit(n_splits=5)
    
    # ä½¿ç”¨ç¤ºä¾‹
    for train_idx, val_idx in kf.split(X):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
        model.fit(X_train, y_train)
        score = model.score(X_val, y_val)
```

## 2. æ·±åº¦å­¦ä¹ 

### 2.1 ç¥ç»ç½‘ç»œåŸºç¡€

**Q: è§£é‡Šåå‘ä¼ æ’­ç®—æ³•çš„åŸç†ï¼Ÿ**

**åå‘ä¼ æ’­æ ¸å¿ƒæ­¥éª¤ï¼š**
1. **å‰å‘ä¼ æ’­**ï¼šè®¡ç®—è¾“å‡ºå’ŒæŸå¤±
2. **åå‘ä¼ æ’­**ï¼šè®¡ç®—æ¢¯åº¦
3. **å‚æ•°æ›´æ–°**ï¼šä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°æƒé‡

**æ•°å­¦åŸç†ï¼š**
```python
import numpy as np

class SimpleNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        # åˆå§‹åŒ–æƒé‡
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def sigmoid_derivative(self, x):
        return x * (1 - x)
    
    def forward(self, X):
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        return self.a2
    
    def backward(self, X, y, output):
        m = X.shape[0]
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        dz2 = output - y
        dW2 = (1/m) * np.dot(self.a1.T, dz2)
        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)
        
        # éšè—å±‚æ¢¯åº¦
        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.a1)
        dW1 = (1/m) * np.dot(X.T, dz1)
        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)
        
        return dW1, db1, dW2, db2
    
    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
```

**Q: å¸¸è§çš„æ¿€æ´»å‡½æ•°æœ‰å“ªäº›ï¼Ÿå„è‡ªçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ**

**æ¿€æ´»å‡½æ•°å¯¹æ¯”ï¼š**

```python
import numpy as np
import matplotlib.pyplot as plt

def activation_functions():
    x = np.linspace(-5, 5, 100)
    
    # Sigmoid
    sigmoid = 1 / (1 + np.exp(-x))
    sigmoid_grad = sigmoid * (1 - sigmoid)
    
    # Tanh
    tanh = np.tanh(x)
    tanh_grad = 1 - tanh**2
    
    # ReLU
    relu = np.maximum(0, x)
    relu_grad = (x > 0).astype(float)
    
    # Leaky ReLU
    leaky_relu = np.where(x > 0, x, 0.01 * x)
    leaky_grad = np.where(x > 0, 1, 0.01)
    
    # ELU
    alpha = 1.0
    elu = np.where(x > 0, x, alpha * (np.exp(x) - 1))
    elu_grad = np.where(x > 0, 1, elu + alpha)
    
    return {
        'sigmoid': (sigmoid, sigmoid_grad),
        'tanh': (tanh, tanh_grad),
        'relu': (relu, relu_grad),
        'leaky_relu': (leaky_relu, leaky_grad),
        'elu': (elu, elu_grad)
    }
```

**æ¿€æ´»å‡½æ•°ç‰¹ç‚¹ï¼š**
- **Sigmoid**ï¼šè¾“å‡º0-1ï¼Œå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **Tanh**ï¼šè¾“å‡º-1åˆ°1ï¼Œæ¯”Sigmoidå¥½ä½†ä»æœ‰æ¢¯åº¦æ¶ˆå¤±
- **ReLU**ï¼šè§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œä½†æœ‰ç¥ç»å…ƒæ­»äº¡é—®é¢˜
- **Leaky ReLU**ï¼šè§£å†³ReLUæ­»äº¡é—®é¢˜
- **ELU**ï¼šç»“åˆReLUå’ŒSigmoidä¼˜ç‚¹

### 2.2 å·ç§¯ç¥ç»ç½‘ç»œ

**Q: è§£é‡Šå·ç§¯å±‚ã€æ± åŒ–å±‚çš„ä½œç”¨å’Œå‚æ•°ï¼Ÿ**

**å·ç§¯å±‚å‚æ•°ï¼š**
- **å·ç§¯æ ¸å¤§å°**ï¼šé€šå¸¸3x3æˆ–5x5
- **æ­¥é•¿(Stride)**ï¼šæ§åˆ¶è¾“å‡ºå°ºå¯¸
- **å¡«å……(Padding)**ï¼šä¿æŒå°ºå¯¸æˆ–å‡å°‘è¾¹ç•Œæ•ˆåº”
- **é€šé“æ•°**ï¼šç‰¹å¾å›¾æ•°é‡

**æ± åŒ–å±‚ä½œç”¨ï¼š**
- é™ç»´å‡å°‘å‚æ•°
- æä¾›å¹³ç§»ä¸å˜æ€§
- æ‰©å¤§æ„Ÿå—é‡

```python
import torch
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        
        # å·ç§¯å—1
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.relu1 = nn.ReLU(inplace=True)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # å·ç§¯å—2
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.relu2 = nn.ReLU(inplace=True)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(64 * 8 * 8, num_classes)
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # å·ç§¯å—1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.pool1(x)
        
        # å·ç§¯å—2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x)
        x = self.pool2(x)
        
        # å±•å¹³
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x
```

### 2.3 å¾ªç¯ç¥ç»ç½‘ç»œ

**Q: LSTMå’ŒGRUçš„åŒºåˆ«å’Œä¼˜åŠ¿ï¼Ÿ**

**LSTMç‰¹ç‚¹ï¼š**
- ä¸‰ä¸ªé—¨ï¼šé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨
- ç»†èƒçŠ¶æ€å’Œéšè—çŠ¶æ€
- æœ‰æ•ˆè§£å†³é•¿æœŸä¾èµ–é—®é¢˜

**GRUç‰¹ç‚¹ï¼š**
- ä¸¤ä¸ªé—¨ï¼šé‡ç½®é—¨ã€æ›´æ–°é—¨
- å‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«
- æ€§èƒ½æ¥è¿‘LSTM

```python
import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_classes):
        super(LSTMModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, 
                           batch_first=True, dropout=0.3)
        self.fc = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        output = self.dropout(lstm_out[:, -1, :])
        output = self.fc(output)
        
        return output

class GRUModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_classes):
        super(GRUModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.gru = nn.GRU(embed_size, hidden_size, num_layers, 
                         batch_first=True, dropout=0.3)
        self.fc = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        embedded = self.embedding(x)
        gru_out, hidden = self.gru(embedded)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        output = self.dropout(gru_out[:, -1, :])
        output = self.fc(output)
        
        return output
```

### 2.4 Transformeræ¶æ„

**Q: è§£é‡ŠTransformerä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ**

**è‡ªæ³¨æ„åŠ›æœºåˆ¶åŸç†ï¼š**
```
Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
```

**å¤šå¤´æ³¨æ„åŠ›å®ç°ï¼š**
```python
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # çº¿æ€§å˜æ¢
        Q = self.W_q(query)
        K = self.W_k(key)
        V = self.W_v(value)
        
        # åˆ†å‰²ä¸ºå¤šå¤´
        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # æ³¨æ„åŠ›è®¡ç®—
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # åˆå¹¶å¤šå¤´
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        
        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.W_o(attention_output)
        
        return output, attention_weights
```

**Transformerä¼˜åŠ¿ï¼š**
- å¹¶è¡Œè®¡ç®—èƒ½åŠ›å¼º
- é•¿è·ç¦»ä¾èµ–å»ºæ¨¡èƒ½åŠ›
- å¯è§£é‡Šæ€§å¥½ï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰
- é¢„è®­ç»ƒæ•ˆæœå¥½